# Sesotho Automatic Tone Extraction — README

This README is a comprehensive, step-by-step playbook for the "Automatic Extraction of Tone for Sesotho" project. It converts the project's plan into actionable commands, file layout, and implementation details so you (or I) can continue work reliably.

---

## TL;DR
- Primary dataset: `Minimal Pairs` (208 WAVs) — use this for initial supervised tone models.
- Secondary dataset: `Processed Recordings` (844 WAVs) — optional augmentation and generalization.
- Reference: Tone-marked dictionaries (16 PDFs) for validation.
- Existing artifacts:
  - `sesotho_tone_manifest.csv` — file manifest (created)
  - `features_sample.csv` — sample features (created)
  - `features_sample_debug.csv` — debug features (created)

Project root: `c:\Users\mubva\Downloads\Nlp`
Notebook: `c:\Users\mubva\Downloads\Nlp\sesotho_tone_extraction_project\sesotho_tone_extraction.ipynb`

---

## Project layout (recommended)
- `sesotho_tone_extraction_project/` — project notebook and README
  - `sesotho_tone_extraction.ipynb` (main notebook)
  - `README.md` (this file)
- `Minimal Pairs Recordings_.../Minimal Pairs/` — primary audio
- `Processed Recordings_.../Processed Recordings/` — supplementary audio
- `Tone Marked Dictionary_.../Dictionary/` — PDFs with tone markings
- Outputs placed in project root (`c:\Users\mubva\Downloads\Nlp`):
  - `sesotho_tone_manifest.csv` (manifest)
  - `features_sample.csv` (sample features)
  - `features_sample_debug.csv` (debug smoke-test)
  - `features_full.parquet` (future; full features)
  - `models/` (saved models)
  - `reports/` (evaluation outputs)

---

## 1 — Environment & reproducibility
Use conda to manage the environment. The commands below are PowerShell-friendly.

Recommended steps (run from PowerShell):

```powershell
# Create environment (optional if using existing conda)
C:/ProgramData/anaconda3/Scripts/conda.exe create -n sesotho-tone python=3.11 -y

# Activate the env
conda activate sesotho-tone

# Install core packages (use conda-forge for audio libs)
C:/ProgramData/anaconda3/Scripts/conda.exe install -n sesotho-tone -c conda-forge librosa pysoundfile soxr numba -y
C:/ProgramData/anaconda3/Scripts/conda.exe install -n sesotho-tone numpy pandas scipy scikit-learn matplotlib seaborn jupyterlab tqdm joblib pyarrow -y

# TensorFlow (CPU; change channel as needed)
C:/ProgramData/anaconda3/Scripts/conda.exe install -n sesotho-tone -c conda-forge tensorflow -y

# Export environment for reproducibility
conda env export -n sesotho-tone > sesotho_tone_env.yml
pip freeze > requirements-pip.txt
```

Notes:
- Use `conda-forge` for `librosa` and audio-related packages (`pysoundfile`, `soxr`).
- If you already installed packages into `c:\Users\mubva\Downloads\Nlp\.conda`, you can keep using that env; just ensure Jupyter kernel points to it.

---

## 2 — Manifest: catalog all audio files
A manifest is already generated by the notebook and saved to:

`c:\Users\mubva\Downloads\Nlp\sesotho_tone_manifest.csv`

Manifest schema (columns):
- `filepath`, `relative_path`, `dataset` (minimal_pairs|processed), `category` (Lexical|Subject Marker|None), `speaker`, `location`, `date`, `segment`

Quick checks (Python):
```python
import pandas as pd
m = pd.read_csv(r"c:\Users\mubva\Downloads\Nlp\sesotho_tone_manifest.csv")
print(m.shape)
print(m['dataset'].value_counts())
m.head()
```

Action: If the manifest is missing files, re-run the manifest cell in `sesotho_tone_extraction.ipynb` and adjust the `PROJECT_ROOT` path.

---

## 3 — Feature extraction (what to compute and how)
We compute per-file aggregated features for fast baseline modelling.

Per-file features (recommended):
- Pitch (F0) statistics: median, mean, std, voiced_ratio using `librosa.pyin` (fallback to `piptrack` when needed).
- MFCCs: 13 coefficients → `mfcc*_mean`, `mfcc*_std` (13 means + 13 stds).
- Spectral: centroid_mean, bandwidth_mean, rolloff_mean (0.85), flatness_mean.
- Energy/RMS: rms_mean, rms_std.
- ZCR: zcr_mean, zcr_std.
- Duration, sample_rate, speaker, label.

Implementation notes (important):
- Resample to a single `sr=16000` for feature consistency: `librosa.load(fp, sr=16000)`.
- `librosa.feature.mfcc` requires keyword arguments: `librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)`.
- Guard `librosa.pyin` outputs for all-NaN — treat unvoiced appropriately and fallback.
- Use `librosa.effects.trim` before feature extraction if recordings have long silences.
- Catch exceptions and append failed files to `features_sample_debug.csv`.

Storage:
- Save small sample as CSV (`features_sample.csv`) for quick checks.
- Save full features in columnar format (`features_full.parquet`) for modeling.

Parallel extraction recommendations:
- Use `joblib.Parallel` or `concurrent.futures.ProcessPoolExecutor`.
- Batch files (e.g., 200 files per write) and append to Parquet to avoid data loss.

---

## 4 — Preprocessing
Steps:
1. Resample (16000 Hz) & convert to mono
2. Trim silence (optional) — keep original durations recorded in manifest
3. Compute features
4. Fill / mark NaN pitch values; compute `voiced_ratio`
5. Standardize features using `StandardScaler` fitted on training split only

Persist scalers for inference: `joblib.dump(scaler, 'models/scaler.joblib')`.

---

## 5 — Labels & mapping
- For initial supervised experiments, label by category folder:
  - Files inside `Lexical` → label = `lexical`
  - Files inside `Subject Marker` → label = `subject_marker`
- If richer tone labels are required, use the Tone Marked Dictionary PDFs as reference; consider manual mapping or semi-automated PDF parsing.

Label sanity:
```python
m['label'] = m['category'].apply(lambda v: 'lexical' if v and 'lexical' in v.lower() else ('subject_marker' if v else None))
m['label'].value_counts()
```

---

## 6 — Train / Validation / Test splits
Split strategies (avoid speaker leakage):
- Use `GroupShuffleSplit` to split by `speaker` so test speakers are unseen: better estimate of generalization.
- Recommended split: 70% train / 15% val / 15% test (grouped by speaker).

Example:
```python
from sklearn.model_selection import GroupShuffleSplit
gss = GroupShuffleSplit(n_splits=1, test_size=0.30, random_state=42)
train_idx, test_idx = next(gss.split(manifest, manifest['label'], groups=manifest['speaker']))
```

---

## 7 — Baseline models
Start with a fast, interpretable baseline:

A. Random Forest (file-level aggregated features)
- Preprocessing: StandardScaler on numerical features
- Model: `RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1)`
- Metrics: accuracy, precision/recall/F1 (macro), confusion matrix per-class

B. SVM baseline (scale features first)

C. Neural approach (later):
- CNN on spectrogram images or LSTM on MFCC sequences
- Use transfer learning with speech models (wav2vec) as embeddings for small classifiers

---

## 8 — Evaluation & error analysis
- Use macro F1 when classes are imbalanced.
- Compute per-speaker and per-location metrics.
- Inspect confusion matrix and misclassified files; listen to audio for qualitative analysis.
- Save evaluation outputs to `reports/eval_metrics.json` and figures to `reports/`.

---

## 9 — Experiment tracking
- Use `tensorboard` (for Keras) and/or `MLflow` for run tracking.
- Save params, metrics, and artifact references (model path, feature file path).

---

## 10 — Troubleshooting (quick answers)
- Error: `mfcc() takes 0 positional arguments` → call `librosa.feature.mfcc(y=y, sr=sr, ...)`.
- Error: `pyin` returns all NaNs → unvoiced content; fallback to `librosa.piptrack` or log as unvoiced.
- If audio loads incorrectly: inspect sample rate and use `sf.info()` or `soundfile` to validate.

---

## 11 — Quick commands & smoke tests
From the project notebook run these cells:
1. Manifest creation cell (already executed): creates `sesotho_tone_manifest.csv`.
2. Feature sample cell (already executed): creates `features_sample.csv` (20 rows).
3. Robust smoke-test cell (already executed): creates `features_sample_debug.csv`.

To inspect outputs (PowerShell / Python):
```powershell
# Show first 20 lines of manifest
Get-Content -Path c:\Users\mubva\Downloads\Nlp\sesotho_tone_manifest.csv -TotalCount 20

# Or in Python
import pandas as pd
pd.read_csv(r"c:\Users\mubva\Downloads\Nlp\features_sample.csv").head(20)
```

---

## 12 — Next actions I can take (pick one)
- [A] Display the first 50 rows of `sesotho_tone_manifest.csv` and `features_sample.csv` here.
- [B] Run full feature extraction for the whole manifest and produce `features_full.parquet` (batched parallel run). This may take time; I will persist intermediate batches.
- [C] Train a RandomForest baseline on `features_sample.csv` now and show results (fast).
- [D] Convert this README into a checked-in file (done — this file) and create `README_short.md` for summary.

Tell me which one you want me to do next and I will run it.

---

## 13 — Contact & notes
If anything in the paths above is changed, update `PROJECT_ROOT` in the notebook before re-running the manifest or feature-extraction cells.

Good — this README has been written to the project folder. If you want, I can now run [A], [B], or [C].
